{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nfrom numpy import array\nimport pandas as pd\nimport re\nimport string as str\n\nimport urllib.request\nfrom bs4 import BeautifulSoup\n\nimport spacy\nimport nltk\nfrom spacy.matcher import Matcher \nfrom spacy.tokens import Span \nfrom nltk.tokenize import word_tokenize\nfrom nltk.tag import pos_tag\nfrom nltk.chunk import conlltags2tree, tree2conlltags\n\nfrom nltk.corpus import stopwords                   # Stopwords corpus\nfrom nltk.stem import PorterStemmer                 # Stemmer\nfrom nltk.stem import WordNetLemmatizer             # WordNet\n\nfrom sklearn.feature_extraction.text import CountVectorizer          #For Bag of words\nfrom sklearn.feature_extraction.text import TfidfVectorizer          #For TF-IDF\nfrom gensim.models import Word2Vec                                   #For Word2Vec\n\nfrom pprint import pprint\nfrom collections import Counter\nfrom tqdm import tqdm\n\nimport networkx as nx\nimport matplotlib.pyplot as plt","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"def url_to_string(url):\n    #res = requests.get(url)\n    #html = res.text\n    html = urllib.request.urlopen(url)\n    \n    soup = BeautifulSoup(html, 'html.parser')\n    \n    # kill all script, style and other elements\n    for script in soup(['script', 'style', 'button', 'a']):\n        script.extract()\n\n    # get text\n    text = soup.get_text()\n        \n    # break into lines and remove leading and trailing space on each\n    lines = [line.strip() for line in text.splitlines()]\n    # break multi-headlines into a line each\n    chunks = [phrase.strip() for line in lines for phrase in line.split(\"  \")]\n    # drop blank lines\n    text = '\\n'.join([chunk for chunk in chunks if chunk])\n    \n    return text\nurl = 'https://insights.hsbc.co.uk/content/hsbc/gb/en_gb/wealth/insights/macro-outlook/china-insights/china-insights-2019-09-11/'\n#test_url = 'https://www.nytimes.com/2018/08/13/us/politics/peter-strzok-fired-fbi.html?hp&action=click&pgtype=Homepage&clickSource=story-heading&module=first-column-region&region=top-news&WT.nav=top-news'\ntext = url_to_string(url)\nprint(text)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.DataFrame({'Sentences':text.split('\\n')})\ndf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['Sentences'][43]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df = df[44:106]\ntrain_df.reset_index(drop=True, inplace=True)\ntrain_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# list of sentences\nsegment = []\nfor text in train_df['Sentences']:\n    segment.append(re.split('[^0-9][\".\"][^0-9]', text))\n\nsentences = [sent for sents in segment for sent in sents]\n#sentences.sort()\nsentences","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Tune some sentences in the passage\nsentences[13] = ' '.join(sentences[13:16])\nsentences.pop(15)\nsentences.pop(14)\nsentences","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def preprocess(sentences):\n    list_of_words = []\n    for sent in sentences:\n        # Convert all words to lowercase\n        sent = sent.lower()\n\n        # Remove numbers\n        # sent = re.sub(r'\\d+', '', sent)\n\n        # Remove punctuation\n        #sent = sent.translate(string.maketrans(','), string.punctuation)\n\n        # Remove space\n        sent = sent.strip()\n\n        # Remove stop-words and Stemming\n        stop_words = set(stopwords.words('english'))\n        tokens = nltk.word_tokenize(sent)\n        tokens_ = [i for i in tokens if not i in stop_words]\n        \n        stemmer = PorterStemmer()\n        for token in tokens_:\n            stems = stemmer.stem(token)\n            list_of_words.append(stems)\n\n        # Lemmatization with WordNet\n    #    lemmatizer = WordNetLemmatizer()\n    #    words = [lemmatizer.lemmatize(word) for word in text.split() if word not in stop_words]\n    \n        #list_of_words.append(words)\n\n    return list_of_words\nlist_of_words = preprocess(sentences)\nlist_of_words","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# POS tagging (input: list of words)\nPOS = nltk.pos_tag(list_of_words)\n\n# Chunking\npattern = 'NP: {<DT>?<JJ>*<NN>}'\n\nNPChunker = nltk.RegexpParser(pattern)\nresult = NPChunker.parse(POS)\nprint(result)\n#result.draw()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Named Entities Recognition\nne_tree = nltk.ne_chunk(POS)\nprint(ne_tree)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"iob_tagged = tree2conlltags(result)\npprint(iob_tagged)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Run NLP model\n#spacy_nlp = spacy.load('en')\nnlp = spacy.load('en_core_web_sm') # load model\n\n# Merge sentences to passage\npassage = '.'.join(sentences)\n\n# create a spaCy object \ndoc = nlp(passage)\n\n# print token, dependency, POS tag \nfor tok in doc:\n    print(tok.text, \"-->\",tok.dep_,\"-->\", tok.pos_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Type of each entities\nfor element in doc.ents:\n    print('Type: %s, Value: %s' % (element.label_, element))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for element in doc.ents:\n    print('Type: %s, Value: %s' % (element.label_, element))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#pprint([(X.text, X.label_) for X in doc.ents])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#pprint([(X, X.ent_iob_, X.ent_type_) for X in doc])\n# B = begins an entity, I = inside an entity, O = outside the entity","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"labels = [x.label_ for x in doc.ents]\nCounter(labels)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"items = [x.text for x in doc.ents]\nCounter(items).most_common(3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#spacy.displacy.render(nlp(sentences[50]), jupyter=True, style='ent')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#spacy.displacy.render(nlp(sentences[50]), style='dep', jupyter = True, options = {'distance': 120})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"[(x.orth_,x.pos_, x.lemma_) for x in [y \n                                      for y\n                                      in nlp(sentences[50]) \n                                      if not y.is_stop and y.pos_ != 'PUNCT']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"spacy.displacy.render(doc, jupyter=True, style='ent')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Simple knowledge graph by getting 2 entities\ndef get_entities(sent):\n    ## chunk 1\n    ent1 = \"\"\n    ent2 = \"\"\n\n    prv_tok_dep = \"\"    # dependency tag of previous token in the sentence\n    prv_tok_text = \"\"   # previous token in the sentence\n\n    prefix = \"\"\n    modifier = \"\"\n  \n    for tok in nlp(sent):\n        ## chunk 2\n        # if token is a punctuation mark then move on to the next token\n        if tok.dep_ != \"punct\":\n            # check: token is a compound word or not\n            if tok.dep_ == \"compound\":\n                prefix = tok.text\n                # if the previous word was also a 'compound' then add the current word to it\n                if prv_tok_dep == \"compound\":\n                    prefix = prv_tok_text + \" \"+ tok.text\n                \n            # check: token is a modifier or not\n            if tok.dep_.endswith(\"mod\") == True:\n                modifier = tok.text\n                # if the previous word was also a 'compound' then add the current word to it\n                if prv_tok_dep == \"compound\":\n                    modifier = prv_tok_text + \" \"+ tok.text\n      \n        \n      \n            ## chunk 3\n            if tok.dep_.find(\"subj\") == True:\n                ent1 = modifier +\" \"+ prefix + \" \"+ tok.text\n                prefix = \"\"\n                modifier = \"\"\n                prv_tok_dep = \"\"\n                prv_tok_text = \"\"\n        \n            ## chunk 4\n            if tok.dep_.find(\"obj\") == True:\n                ent2 = modifier +\" \"+ prefix +\" \"+ tok.text\n        \n            ## chunk 5  \n            # update variables\n            prv_tok_dep = tok.dep_\n            prv_tok_text = tok.text\n\n    return [ent1.strip(), ent2.strip()]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"entity_pairs = []\n\nfor i in sentences:\n    entity_pairs.append(get_entities(i))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"entity_pairs","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_relation(sent):\n    doc = nlp(sent)\n    # Matcher class object \n    matcher = Matcher(nlp.vocab)\n    #define the pattern \n    pattern = [{'DEP':'ROOT'}, \n            {'DEP':'prep','OP':\"?\"},\n            {'DEP':'agent','OP':\"?\"},  \n            {'POS':'ADJ','OP':\"?\"}] \n    matcher.add(\"matching_1\", None, pattern) \n    matches = matcher(doc)\n    k = len(matches) - 1\n\n    span = doc[matches[k][1]:matches[k][2]] \n\n    return(span.text)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"relations = [get_relation(i) for i in tqdm(sentences)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.Series(relations).value_counts()[:50]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# extract subject\nsource = [i[0] for i in entity_pairs]\n\n# extract object\ntarget = [i[1] for i in entity_pairs]\n\nkg_df = pd.DataFrame({'source':source, 'target':target, 'edge':relations})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# create a directed-graph from a dataframe\nG=nx.from_pandas_edgelist(kg_df, \"source\", \"target\", \n                          edge_attr=True, create_using=nx.MultiDiGraph())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12,12))\n\npos = nx.spring_layout(G)\nnx.draw(G, with_labels=True, node_color='skyblue', edge_cmap=plt.cm.Blues, pos = pos)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"G=nx.from_pandas_edgelist(kg_df[kg_df['edge']==\"accepts\"], \"source\", \"target\", \n                          edge_attr=True, create_using=nx.MultiDiGraph())\n\nplt.figure(figsize=(12,12))\npos = nx.spring_layout(G, k = 0.5) # k regulates the distance between nodes\nnx.draw(G, with_labels=True, node_color='skyblue', node_size=1500, edge_cmap=plt.cm.Blues, pos = pos)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}